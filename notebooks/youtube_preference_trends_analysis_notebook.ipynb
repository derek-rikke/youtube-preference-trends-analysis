{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39005ac9",
   "metadata": {},
   "source": [
    "# ðŸŽ¥ YouTube Audience Trends Analysis\n",
    "\n",
    "**Objective:**  \n",
    "Analyze how YouTube audience preferences evolve over time using trending video data. We'll explore changes in engagement, content types, and metadata patterns across different countries and timeframes.\n",
    "\n",
    "**Dataset:**  \n",
    "[YouTube Trending Video Statistics (Kaggle)](https://www.kaggle.com/datasets/datasnaek/youtube-new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4f214f",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries\n",
    "\n",
    "We begin by importing all necessary Python libraries for the **ETL process** â€” including tools for file management, data manipulation, visualization, environment variable handling, and accessing the Kaggle API to download the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "465b39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not already installed, run this in a separate cell:\n",
    "# !pip install kaggle python-dotenv pandas numpy matplotlib seaborn\n",
    "\n",
    "# File handling & environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Data manipulation & exploration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Kaggle API\n",
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "# Plot settings\n",
    "sns.set(style='whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3969bef2",
   "metadata": {},
   "source": [
    "### Step 2: Extract YouTube Trending Dataset from Kaggle\n",
    "\n",
    "We load our Kaggle authentication credentials from the `.env` file and use the Kaggle API to download the trending video dataset published by [datasnaek](https://www.kaggle.com/datasets/datasnaek/youtube-new). This dataset includes trending videos from six countries, including the US, UK, Canada, and India."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9f37e004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/datasnaek/youtube-new\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set Kaggle credentials\n",
    "os.environ['KAGGLE_USERNAME'] = os.getenv('KAGGLE_USERNAME')\n",
    "os.environ['KAGGLE_KEY'] = os.getenv('KAGGLE_KEY')\n",
    "\n",
    "# Authenticate and download dataset\n",
    "api = KaggleApi()\n",
    "api.authenticate()\n",
    "api.dataset_download_files(\n",
    "    'datasnaek/youtube-new',\n",
    "    path='youtube_data',\n",
    "    unzip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbefb86",
   "metadata": {},
   "source": [
    "### Step 3: Load and Inspect the Raw YouTube Dataset\n",
    "\n",
    "We'll begin by listing all of the files in the downloaded folder and then loading the US trending videos dataset (`USvideos.csv`) as our working subset. This initial inspection will help us understand the structure of the data, identify key fields, and begin planning our cleaning steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a07dbf62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in dataset folder: ['CAvideos.csv', 'CA_category_id.json', 'DEvideos.csv', 'DE_category_id.json', 'FRvideos.csv', 'FR_category_id.json', 'GBvideos.csv', 'GB_category_id.json', 'INvideos.csv', 'IN_category_id.json', 'JPvideos.csv', 'JP_category_id.json', 'KRvideos.csv', 'KR_category_id.json', 'MXvideos.csv', 'MX_category_id.json', 'RUvideos.csv', 'RU_category_id.json', 'USvideos.csv', 'US_category_id.json']\n",
      "Shape of dataset: (40949, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>comment_count</th>\n",
       "      <th>thumbnail_link</th>\n",
       "      <th>comments_disabled</th>\n",
       "      <th>ratings_disabled</th>\n",
       "      <th>video_error_or_removed</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2kyS6SvSYSE</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n",
       "      <td>CaseyNeistat</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-11-13T17:13:01.000Z</td>\n",
       "      <td>SHANtell martin</td>\n",
       "      <td>748374</td>\n",
       "      <td>57527</td>\n",
       "      <td>2966</td>\n",
       "      <td>15954</td>\n",
       "      <td>https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>SHANTELL'S CHANNEL - https://www.youtube.com/s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ZAPwfrtAFY</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>The Trump Presidency: Last Week Tonight with J...</td>\n",
       "      <td>LastWeekTonight</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13T07:30:00.000Z</td>\n",
       "      <td>last week tonight trump presidency|\"last week ...</td>\n",
       "      <td>2418783</td>\n",
       "      <td>97185</td>\n",
       "      <td>6146</td>\n",
       "      <td>12703</td>\n",
       "      <td>https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>One year after the presidential election, John...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5qpjK5DgCt4</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Racist Superman | Rudy Mancuso, King Bach &amp; Le...</td>\n",
       "      <td>Rudy Mancuso</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-11-12T19:05:24.000Z</td>\n",
       "      <td>racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...</td>\n",
       "      <td>3191434</td>\n",
       "      <td>146033</td>\n",
       "      <td>5339</td>\n",
       "      <td>8181</td>\n",
       "      <td>https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>WATCH MY PREVIOUS VIDEO â–¶ \\n\\nSUBSCRIBE â–º http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>puqaWrEC7tY</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>Nickelback Lyrics: Real or Fake?</td>\n",
       "      <td>Good Mythical Morning</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13T11:00:04.000Z</td>\n",
       "      <td>rhett and link|\"gmm\"|\"good mythical morning\"|\"...</td>\n",
       "      <td>343168</td>\n",
       "      <td>10172</td>\n",
       "      <td>666</td>\n",
       "      <td>2146</td>\n",
       "      <td>https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>Today we find out if Link is a Nickelback amat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d380meD0W0M</td>\n",
       "      <td>17.14.11</td>\n",
       "      <td>I Dare You: GOING BALD!?</td>\n",
       "      <td>nigahiga</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-12T18:01:41.000Z</td>\n",
       "      <td>ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"...</td>\n",
       "      <td>2095731</td>\n",
       "      <td>132235</td>\n",
       "      <td>1989</td>\n",
       "      <td>17518</td>\n",
       "      <td>https://i.ytimg.com/vi/d380meD0W0M/default.jpg</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>I know it's been a while since we did this sho...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id trending_date  \\\n",
       "0  2kyS6SvSYSE      17.14.11   \n",
       "1  1ZAPwfrtAFY      17.14.11   \n",
       "2  5qpjK5DgCt4      17.14.11   \n",
       "3  puqaWrEC7tY      17.14.11   \n",
       "4  d380meD0W0M      17.14.11   \n",
       "\n",
       "                                               title          channel_title  \\\n",
       "0                 WE WANT TO TALK ABOUT OUR MARRIAGE           CaseyNeistat   \n",
       "1  The Trump Presidency: Last Week Tonight with J...        LastWeekTonight   \n",
       "2  Racist Superman | Rudy Mancuso, King Bach & Le...           Rudy Mancuso   \n",
       "3                   Nickelback Lyrics: Real or Fake?  Good Mythical Morning   \n",
       "4                           I Dare You: GOING BALD!?               nigahiga   \n",
       "\n",
       "   category_id              publish_time  \\\n",
       "0           22  2017-11-13T17:13:01.000Z   \n",
       "1           24  2017-11-13T07:30:00.000Z   \n",
       "2           23  2017-11-12T19:05:24.000Z   \n",
       "3           24  2017-11-13T11:00:04.000Z   \n",
       "4           24  2017-11-12T18:01:41.000Z   \n",
       "\n",
       "                                                tags    views   likes  \\\n",
       "0                                    SHANtell martin   748374   57527   \n",
       "1  last week tonight trump presidency|\"last week ...  2418783   97185   \n",
       "2  racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...  3191434  146033   \n",
       "3  rhett and link|\"gmm\"|\"good mythical morning\"|\"...   343168   10172   \n",
       "4  ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"...  2095731  132235   \n",
       "\n",
       "   dislikes  comment_count                                  thumbnail_link  \\\n",
       "0      2966          15954  https://i.ytimg.com/vi/2kyS6SvSYSE/default.jpg   \n",
       "1      6146          12703  https://i.ytimg.com/vi/1ZAPwfrtAFY/default.jpg   \n",
       "2      5339           8181  https://i.ytimg.com/vi/5qpjK5DgCt4/default.jpg   \n",
       "3       666           2146  https://i.ytimg.com/vi/puqaWrEC7tY/default.jpg   \n",
       "4      1989          17518  https://i.ytimg.com/vi/d380meD0W0M/default.jpg   \n",
       "\n",
       "   comments_disabled  ratings_disabled  video_error_or_removed  \\\n",
       "0              False             False                   False   \n",
       "1              False             False                   False   \n",
       "2              False             False                   False   \n",
       "3              False             False                   False   \n",
       "4              False             False                   False   \n",
       "\n",
       "                                         description  \n",
       "0  SHANTELL'S CHANNEL - https://www.youtube.com/s...  \n",
       "1  One year after the presidential election, John...  \n",
       "2  WATCH MY PREVIOUS VIDEO â–¶ \\n\\nSUBSCRIBE â–º http...  \n",
       "3  Today we find out if Link is a Nickelback amat...  \n",
       "4  I know it's been a while since we did this sho...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List all files in the dataset folder\n",
    "files = os.listdir('youtube_data')\n",
    "print(\"Files in dataset folder:\", files)\n",
    "\n",
    "# Load the US trending videos dataset\n",
    "df_us = pd.read_csv(os.path.join('youtube_data', 'USvideos.csv'))\n",
    "\n",
    "# Preview structure\n",
    "print(\"Shape of dataset:\", df_us.shape)\n",
    "df_us.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af30a0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40949 entries, 0 to 40948\n",
      "Data columns (total 16 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   video_id                40949 non-null  object\n",
      " 1   trending_date           40949 non-null  object\n",
      " 2   title                   40949 non-null  object\n",
      " 3   channel_title           40949 non-null  object\n",
      " 4   category_id             40949 non-null  int64 \n",
      " 5   publish_time            40949 non-null  object\n",
      " 6   tags                    40949 non-null  object\n",
      " 7   views                   40949 non-null  int64 \n",
      " 8   likes                   40949 non-null  int64 \n",
      " 9   dislikes                40949 non-null  int64 \n",
      " 10  comment_count           40949 non-null  int64 \n",
      " 11  thumbnail_link          40949 non-null  object\n",
      " 12  comments_disabled       40949 non-null  bool  \n",
      " 13  ratings_disabled        40949 non-null  bool  \n",
      " 14  video_error_or_removed  40949 non-null  bool  \n",
      " 15  description             40379 non-null  object\n",
      "dtypes: bool(3), int64(5), object(8)\n",
      "memory usage: 4.2+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "video_id                    0\n",
       "trending_date               0\n",
       "title                       0\n",
       "channel_title               0\n",
       "category_id                 0\n",
       "publish_time                0\n",
       "tags                        0\n",
       "views                       0\n",
       "likes                       0\n",
       "dislikes                    0\n",
       "comment_count               0\n",
       "thumbnail_link              0\n",
       "comments_disabled           0\n",
       "ratings_disabled            0\n",
       "video_error_or_removed      0\n",
       "description               570\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check data types and nulls\n",
    "df_us.info()\n",
    "df_us.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7fd878",
   "metadata": {},
   "source": [
    "### Step 4: Transform and Enrich the US Dataset\n",
    "\n",
    "Now that we've previewed the US trending dataset, we'll apply a full set of transformations to prepare it for analysis. These changes focus on time parsing, categorical mapping, and extracting new features that will help us explore audience preferences and cultural trends.\n",
    "\n",
    "#### Transformations performed:\n",
    "- Convert `publish_time` and `trending_date` to datetime\n",
    "- Extract time-related features (e.g., publish/trending hour, weekday, etc.)\n",
    "- Map `category_id` to readable `category_name` using the provided JSON file\n",
    "- Clean the `title` column and derive:\n",
    "  - `title_length`\n",
    "  - `has_emoji`\n",
    "  - `has_question`\n",
    "- Add `tag_count` for analysis of metadata use\n",
    "- Prepare the final cleaned DataFrame `df_us_clean` for export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d4cec87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>...</th>\n",
       "      <th>publish_hour</th>\n",
       "      <th>publish_dayofweek</th>\n",
       "      <th>trending_month</th>\n",
       "      <th>trending_year</th>\n",
       "      <th>trending_dayofweek</th>\n",
       "      <th>category_name</th>\n",
       "      <th>title_length</th>\n",
       "      <th>has_emoji</th>\n",
       "      <th>has_question</th>\n",
       "      <th>tag_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2kyS6SvSYSE</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n",
       "      <td>CaseyNeistat</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-11-13 17:13:01+00:00</td>\n",
       "      <td>SHANtell martin</td>\n",
       "      <td>748374</td>\n",
       "      <td>57527</td>\n",
       "      <td>2966</td>\n",
       "      <td>...</td>\n",
       "      <td>17</td>\n",
       "      <td>Monday</td>\n",
       "      <td>November</td>\n",
       "      <td>2017</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "      <td>34</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ZAPwfrtAFY</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>The Trump Presidency: Last Week Tonight with J...</td>\n",
       "      <td>LastWeekTonight</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13 07:30:00+00:00</td>\n",
       "      <td>last week tonight trump presidency|\"last week ...</td>\n",
       "      <td>2418783</td>\n",
       "      <td>97185</td>\n",
       "      <td>6146</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>Monday</td>\n",
       "      <td>November</td>\n",
       "      <td>2017</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>62</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5qpjK5DgCt4</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>Racist Superman | Rudy Mancuso, King Bach &amp; Le...</td>\n",
       "      <td>Rudy Mancuso</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-11-12 19:05:24+00:00</td>\n",
       "      <td>racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...</td>\n",
       "      <td>3191434</td>\n",
       "      <td>146033</td>\n",
       "      <td>5339</td>\n",
       "      <td>...</td>\n",
       "      <td>19</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>November</td>\n",
       "      <td>2017</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>53</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>puqaWrEC7tY</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>Nickelback Lyrics: Real or Fake?</td>\n",
       "      <td>Good Mythical Morning</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13 11:00:04+00:00</td>\n",
       "      <td>rhett and link|\"gmm\"|\"good mythical morning\"|\"...</td>\n",
       "      <td>343168</td>\n",
       "      <td>10172</td>\n",
       "      <td>666</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>Monday</td>\n",
       "      <td>November</td>\n",
       "      <td>2017</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d380meD0W0M</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>I Dare You: GOING BALD!?</td>\n",
       "      <td>nigahiga</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-12 18:01:41+00:00</td>\n",
       "      <td>ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"...</td>\n",
       "      <td>2095731</td>\n",
       "      <td>132235</td>\n",
       "      <td>1989</td>\n",
       "      <td>...</td>\n",
       "      <td>18</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>November</td>\n",
       "      <td>2017</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id trending_date  \\\n",
       "0  2kyS6SvSYSE    2017-11-14   \n",
       "1  1ZAPwfrtAFY    2017-11-14   \n",
       "2  5qpjK5DgCt4    2017-11-14   \n",
       "3  puqaWrEC7tY    2017-11-14   \n",
       "4  d380meD0W0M    2017-11-14   \n",
       "\n",
       "                                               title          channel_title  \\\n",
       "0                 WE WANT TO TALK ABOUT OUR MARRIAGE           CaseyNeistat   \n",
       "1  The Trump Presidency: Last Week Tonight with J...        LastWeekTonight   \n",
       "2  Racist Superman | Rudy Mancuso, King Bach & Le...           Rudy Mancuso   \n",
       "3                   Nickelback Lyrics: Real or Fake?  Good Mythical Morning   \n",
       "4                           I Dare You: GOING BALD!?               nigahiga   \n",
       "\n",
       "   category_id              publish_time  \\\n",
       "0           22 2017-11-13 17:13:01+00:00   \n",
       "1           24 2017-11-13 07:30:00+00:00   \n",
       "2           23 2017-11-12 19:05:24+00:00   \n",
       "3           24 2017-11-13 11:00:04+00:00   \n",
       "4           24 2017-11-12 18:01:41+00:00   \n",
       "\n",
       "                                                tags    views   likes  \\\n",
       "0                                    SHANtell martin   748374   57527   \n",
       "1  last week tonight trump presidency|\"last week ...  2418783   97185   \n",
       "2  racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...  3191434  146033   \n",
       "3  rhett and link|\"gmm\"|\"good mythical morning\"|\"...   343168   10172   \n",
       "4  ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"...  2095731  132235   \n",
       "\n",
       "   dislikes  ...  publish_hour publish_dayofweek  trending_month  \\\n",
       "0      2966  ...            17            Monday        November   \n",
       "1      6146  ...             7            Monday        November   \n",
       "2      5339  ...            19            Sunday        November   \n",
       "3       666  ...            11            Monday        November   \n",
       "4      1989  ...            18            Sunday        November   \n",
       "\n",
       "   trending_year  trending_dayofweek   category_name title_length  has_emoji  \\\n",
       "0           2017             Tuesday  People & Blogs           34      False   \n",
       "1           2017             Tuesday   Entertainment           62      False   \n",
       "2           2017             Tuesday          Comedy           53      False   \n",
       "3           2017             Tuesday   Entertainment           32      False   \n",
       "4           2017             Tuesday   Entertainment           24      False   \n",
       "\n",
       "  has_question tag_count  \n",
       "0        False         1  \n",
       "1        False         4  \n",
       "2        False        23  \n",
       "3         True        27  \n",
       "4         True        14  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import emoji\n",
    "import re\n",
    "\n",
    "# ---------- Date Conversion ----------\n",
    "df_us['publish_time'] = pd.to_datetime(df_us['publish_time'])\n",
    "df_us['trending_date'] = pd.to_datetime(df_us['trending_date'], format='%y.%d.%m')\n",
    "\n",
    "# ---------- Time Features ----------\n",
    "df_us['publish_hour'] = df_us['publish_time'].dt.hour\n",
    "df_us['publish_dayofweek'] = df_us['publish_time'].dt.day_name()\n",
    "df_us['trending_year'] = df_us['trending_date'].dt.year\n",
    "df_us['trending_month'] = df_us['trending_date'].dt.month_name()\n",
    "df_us['trending_dayofweek'] = df_us['trending_date'].dt.day_name()\n",
    "\n",
    "# ---------- Category Mapping ----------\n",
    "with open('youtube_data/US_category_id.json', 'r') as f:\n",
    "    category_data = json.load(f)\n",
    "\n",
    "# Flatten JSON to get category_id â†’ category_name mapping\n",
    "id_to_name = {int(item['id']): item['snippet']['title'] for item in category_data['items']}\n",
    "df_us['category_name'] = df_us['category_id'].map(id_to_name)\n",
    "\n",
    "# ---------- Title Cleaning + Feature Engineering ----------\n",
    "df_us['title'] = df_us['title'].str.strip()\n",
    "\n",
    "# Title length\n",
    "df_us['title_length'] = df_us['title'].str.len()\n",
    "\n",
    "# Contains emoji\n",
    "df_us['has_emoji'] = df_us['title'].apply(lambda x: any(char in emoji.EMOJI_DATA for char in x))\n",
    "\n",
    "# Contains question mark\n",
    "df_us['has_question'] = df_us['title'].str.contains(r'\\?', regex=True)\n",
    "\n",
    "# ---------- Tag Feature ----------\n",
    "df_us['tag_count'] = df_us['tags'].apply(lambda x: 0 if x == '[none]' else len(x.split('|')))\n",
    "\n",
    "# ---------- Final Preview ----------\n",
    "df_us_clean = df_us.copy()\n",
    "df_us_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab15a98",
   "metadata": {},
   "source": [
    "### Step 5: Review Feature Enrichment and Save Cleaned US Dataset\n",
    "\n",
    "Before moving on to processing the other country datasets, we briefly review the enriched features in the US dataset to confirm that the transformations were successful and meaningful.\n",
    "\n",
    "#### Notable observations:\n",
    "- `publish_dayofweek` and `trending_dayofweek` columns show the weekday patterns of uploads vs when they trend.\n",
    "- `has_emoji` appears to be rare in this early subset, but we can analyze its rise in later years.\n",
    "- `has_question` is true for some titles, indicating variation in tone and style â€” possibly linked to clickbait or audience engagement tactics.\n",
    "- `tag_count` varies widely â€” from a single keyword to over 20 â€” hinting at differing metadata strategies by creators.\n",
    "\n",
    "We now save this cleaned US dataset to a CSV file so it can be reused for Tableau visualization or combined with other cleaned datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e91fe6ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… US cleaned dataset saved to 'clean_data/US_clean.csv'\n"
     ]
    }
   ],
   "source": [
    "# Ensure output directory exists\n",
    "os.makedirs('clean_data', exist_ok=True)\n",
    "\n",
    "# Save cleaned US dataset\n",
    "df_us_clean.to_csv('clean_data/US_clean.csv', index=False)\n",
    "print(\"âœ… US cleaned dataset saved to 'clean_data/US_clean.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce836d4",
   "metadata": {},
   "source": [
    "### Step 6: Clean and Combine All Country Datasets\n",
    "\n",
    "Now that we've verified the enrichment process on the US dataset, we'll apply the same transformations to all other country files. This includes:\n",
    "\n",
    "- Parsing date and time fields\n",
    "- Mapping category IDs to names\n",
    "- Extracting title and tag-based features\n",
    "- Adding a `country` column to each dataset\n",
    "\n",
    "We'll skip the already-processed US file, clean the remaining datasets in batch, and then combine all of them into a single master file for cross-country analysis. Each cleaned country file will also be saved individually in the `clean_data/` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f0ac504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Processing CA...\n",
      "âœ… Saved cleaned CA data to clean_data/CA_clean.csv\n",
      "ðŸ”„ Processing DE...\n",
      "âœ… Saved cleaned DE data to clean_data/DE_clean.csv\n",
      "ðŸ”„ Processing FR...\n",
      "âœ… Saved cleaned FR data to clean_data/FR_clean.csv\n",
      "ðŸ”„ Processing GB...\n",
      "âœ… Saved cleaned GB data to clean_data/GB_clean.csv\n",
      "ðŸ”„ Processing IN...\n",
      "âœ… Saved cleaned IN data to clean_data/IN_clean.csv\n",
      "ðŸ”„ Processing JP...\n",
      "âœ… Saved cleaned JP data to clean_data/JP_clean.csv\n",
      "ðŸ”„ Processing KR...\n",
      "âœ… Saved cleaned KR data to clean_data/KR_clean.csv\n",
      "ðŸ”„ Processing MX...\n",
      "âœ… Saved cleaned MX data to clean_data/MX_clean.csv\n",
      "ðŸ”„ Processing RU...\n",
      "âœ… Saved cleaned RU data to clean_data/RU_clean.csv\n",
      "âœ… Combined dataset saved to 'clean_data/all_countries_clean.csv'\n"
     ]
    }
   ],
   "source": [
    "def clean_country_df(file_path, country_code, category_map):\n",
    "    df = df = pd.read_csv(file_path, encoding='utf-8', encoding_errors='replace')\n",
    "\n",
    "    # Parse dates\n",
    "    df['publish_time'] = pd.to_datetime(df['publish_time'])\n",
    "    df['trending_date'] = pd.to_datetime(df['trending_date'], format='%y.%d.%m')\n",
    "\n",
    "    # Time features\n",
    "    df['publish_hour'] = df['publish_time'].dt.hour\n",
    "    df['publish_dayofweek'] = df['publish_time'].dt.day_name()\n",
    "    df['trending_year'] = df['trending_date'].dt.year\n",
    "    df['trending_month'] = df['trending_date'].dt.month_name()\n",
    "    df['trending_dayofweek'] = df['trending_date'].dt.day_name()\n",
    "\n",
    "    # Map category ID to name\n",
    "    df['category_name'] = df['category_id'].map(category_map)\n",
    "\n",
    "    # Title features\n",
    "    df['title'] = df['title'].str.strip()\n",
    "    df['title_length'] = df['title'].str.len()\n",
    "    df['has_emoji'] = df['title'].apply(lambda x: any(char in emoji.EMOJI_DATA for char in x))\n",
    "    df['has_question'] = df['title'].str.contains(r'\\?', regex=True)\n",
    "\n",
    "    # Tag features\n",
    "    df['tag_count'] = df['tags'].apply(lambda x: 0 if x == '[none]' else len(x.split('|')))\n",
    "\n",
    "    # Country label\n",
    "    df['country'] = country_code\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# Load category map (same for all countries)\n",
    "with open('youtube_data/US_category_id.json', 'r') as f:\n",
    "    category_data = json.load(f)\n",
    "category_map = {int(item['id']): item['snippet']['title'] for item in category_data['items']}\n",
    "\n",
    "# Get list of country files\n",
    "all_files = os.listdir('youtube_data')\n",
    "country_files = [f for f in all_files if f.endswith('.csv') and not f.startswith('US')]\n",
    "\n",
    "# Process each file\n",
    "clean_dfs = []\n",
    "for filename in country_files:\n",
    "    country_code = filename[:2].upper()\n",
    "    print(f\"ðŸ”„ Processing {country_code}...\")\n",
    "    df_clean = clean_country_df(os.path.join('youtube_data', filename), country_code, category_map)\n",
    "    clean_dfs.append(df_clean)\n",
    "\n",
    "    # Save individual country CSV\n",
    "    output_path = f'clean_data/{country_code}_clean.csv'\n",
    "    df_clean.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Saved cleaned {country_code} data to {output_path}\")\n",
    "\n",
    "# Combine with US cleaned dataset\n",
    "df_us_clean['country'] = 'US'  # Ensure US has country column\n",
    "df_all = pd.concat([df_us_clean] + clean_dfs, ignore_index=True)\n",
    "\n",
    "# Save combined dataset\n",
    "df_all.to_csv('clean_data/all_countries_clean.csv', index=False)\n",
    "print(\"âœ… Combined dataset saved to 'clean_data/all_countries_clean.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3091b",
   "metadata": {},
   "source": [
    "### Step 7: Validate the Combined Dataset\n",
    "\n",
    "Before exporting the final dataset for visualization, we perform a few quick checks to confirm the success of the cleaning process across all countries:\n",
    "\n",
    "- Preview the combined dataset\n",
    "- Check the presence and consistency of key columns\n",
    "- Validate country distribution\n",
    "- Look for unexpected nulls or formatting issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e37812c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of combined dataset: (375942, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>trending_date</th>\n",
       "      <th>title</th>\n",
       "      <th>channel_title</th>\n",
       "      <th>category_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>tags</th>\n",
       "      <th>views</th>\n",
       "      <th>likes</th>\n",
       "      <th>dislikes</th>\n",
       "      <th>...</th>\n",
       "      <th>publish_dayofweek</th>\n",
       "      <th>trending_month</th>\n",
       "      <th>trending_year</th>\n",
       "      <th>trending_dayofweek</th>\n",
       "      <th>category_name</th>\n",
       "      <th>title_length</th>\n",
       "      <th>has_emoji</th>\n",
       "      <th>has_question</th>\n",
       "      <th>tag_count</th>\n",
       "      <th>country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2kyS6SvSYSE</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>WE WANT TO TALK ABOUT OUR MARRIAGE</td>\n",
       "      <td>CaseyNeistat</td>\n",
       "      <td>22</td>\n",
       "      <td>2017-11-13 17:13:01+00:00</td>\n",
       "      <td>SHANtell martin</td>\n",
       "      <td>748374</td>\n",
       "      <td>57527</td>\n",
       "      <td>2966</td>\n",
       "      <td>...</td>\n",
       "      <td>Monday</td>\n",
       "      <td>November</td>\n",
       "      <td>2017</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>People &amp; Blogs</td>\n",
       "      <td>34</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ZAPwfrtAFY</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>The Trump Presidency: Last Week Tonight with J...</td>\n",
       "      <td>LastWeekTonight</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13 07:30:00+00:00</td>\n",
       "      <td>last week tonight trump presidency|\"last week ...</td>\n",
       "      <td>2418783</td>\n",
       "      <td>97185</td>\n",
       "      <td>6146</td>\n",
       "      <td>...</td>\n",
       "      <td>Monday</td>\n",
       "      <td>November</td>\n",
       "      <td>2017</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>62</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>4</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5qpjK5DgCt4</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>Racist Superman | Rudy Mancuso, King Bach &amp; Le...</td>\n",
       "      <td>Rudy Mancuso</td>\n",
       "      <td>23</td>\n",
       "      <td>2017-11-12 19:05:24+00:00</td>\n",
       "      <td>racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...</td>\n",
       "      <td>3191434</td>\n",
       "      <td>146033</td>\n",
       "      <td>5339</td>\n",
       "      <td>...</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>November</td>\n",
       "      <td>2017</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Comedy</td>\n",
       "      <td>53</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>23</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>puqaWrEC7tY</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>Nickelback Lyrics: Real or Fake?</td>\n",
       "      <td>Good Mythical Morning</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-13 11:00:04+00:00</td>\n",
       "      <td>rhett and link|\"gmm\"|\"good mythical morning\"|\"...</td>\n",
       "      <td>343168</td>\n",
       "      <td>10172</td>\n",
       "      <td>666</td>\n",
       "      <td>...</td>\n",
       "      <td>Monday</td>\n",
       "      <td>November</td>\n",
       "      <td>2017</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>32</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>27</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d380meD0W0M</td>\n",
       "      <td>2017-11-14</td>\n",
       "      <td>I Dare You: GOING BALD!?</td>\n",
       "      <td>nigahiga</td>\n",
       "      <td>24</td>\n",
       "      <td>2017-11-12 18:01:41+00:00</td>\n",
       "      <td>ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"...</td>\n",
       "      <td>2095731</td>\n",
       "      <td>132235</td>\n",
       "      <td>1989</td>\n",
       "      <td>...</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>November</td>\n",
       "      <td>2017</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>24</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>14</td>\n",
       "      <td>US</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      video_id trending_date  \\\n",
       "0  2kyS6SvSYSE    2017-11-14   \n",
       "1  1ZAPwfrtAFY    2017-11-14   \n",
       "2  5qpjK5DgCt4    2017-11-14   \n",
       "3  puqaWrEC7tY    2017-11-14   \n",
       "4  d380meD0W0M    2017-11-14   \n",
       "\n",
       "                                               title          channel_title  \\\n",
       "0                 WE WANT TO TALK ABOUT OUR MARRIAGE           CaseyNeistat   \n",
       "1  The Trump Presidency: Last Week Tonight with J...        LastWeekTonight   \n",
       "2  Racist Superman | Rudy Mancuso, King Bach & Le...           Rudy Mancuso   \n",
       "3                   Nickelback Lyrics: Real or Fake?  Good Mythical Morning   \n",
       "4                           I Dare You: GOING BALD!?               nigahiga   \n",
       "\n",
       "   category_id              publish_time  \\\n",
       "0           22 2017-11-13 17:13:01+00:00   \n",
       "1           24 2017-11-13 07:30:00+00:00   \n",
       "2           23 2017-11-12 19:05:24+00:00   \n",
       "3           24 2017-11-13 11:00:04+00:00   \n",
       "4           24 2017-11-12 18:01:41+00:00   \n",
       "\n",
       "                                                tags    views   likes  \\\n",
       "0                                    SHANtell martin   748374   57527   \n",
       "1  last week tonight trump presidency|\"last week ...  2418783   97185   \n",
       "2  racist superman|\"rudy\"|\"mancuso\"|\"king\"|\"bach\"...  3191434  146033   \n",
       "3  rhett and link|\"gmm\"|\"good mythical morning\"|\"...   343168   10172   \n",
       "4  ryan|\"higa\"|\"higatv\"|\"nigahiga\"|\"i dare you\"|\"...  2095731  132235   \n",
       "\n",
       "   dislikes  ...  publish_dayofweek trending_month  trending_year  \\\n",
       "0      2966  ...             Monday       November           2017   \n",
       "1      6146  ...             Monday       November           2017   \n",
       "2      5339  ...             Sunday       November           2017   \n",
       "3       666  ...             Monday       November           2017   \n",
       "4      1989  ...             Sunday       November           2017   \n",
       "\n",
       "   trending_dayofweek   category_name title_length has_emoji  has_question  \\\n",
       "0             Tuesday  People & Blogs           34     False         False   \n",
       "1             Tuesday   Entertainment           62     False         False   \n",
       "2             Tuesday          Comedy           53     False         False   \n",
       "3             Tuesday   Entertainment           32     False          True   \n",
       "4             Tuesday   Entertainment           24     False          True   \n",
       "\n",
       "  tag_count country  \n",
       "0         1      US  \n",
       "1         4      US  \n",
       "2        23      US  \n",
       "3        27      US  \n",
       "4        14      US  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview rows and shape\n",
    "print(\"Shape of combined dataset:\", df_all.shape)\n",
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "036256e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries included: ['US' 'CA' 'DE' 'FR' 'GB' 'IN' 'JP' 'KR' 'MX' 'RU']\n",
      "\n",
      "Rows per country:\n",
      "country\n",
      "US    40949\n",
      "CA    40881\n",
      "DE    40840\n",
      "RU    40739\n",
      "FR    40724\n",
      "MX    40451\n",
      "GB    38916\n",
      "IN    37352\n",
      "KR    34567\n",
      "JP    20523\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Missing values in key fields:\n",
      "title            0\n",
      "publish_time     0\n",
      "trending_date    0\n",
      "category_name    0\n",
      "dtype: int64\n",
      "\n",
      "Has emoji breakdown:\n",
      "has_emoji\n",
      "False    364824\n",
      "True      11118\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Average title length per country:\n",
      "country\n",
      "CA    53.709254\n",
      "DE    55.407542\n",
      "FR    53.717562\n",
      "GB    49.549774\n",
      "IN    70.563022\n",
      "JP    37.880817\n",
      "KR    41.244395\n",
      "MX    58.042891\n",
      "RU    53.124058\n",
      "US    48.578183\n",
      "Name: title_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Confirm all countries present\n",
    "print(\"Countries included:\", df_all['country'].unique())\n",
    "\n",
    "# Check that each country has data\n",
    "print(\"\\nRows per country:\")\n",
    "print(df_all['country'].value_counts())\n",
    "\n",
    "# Check for nulls in important columns\n",
    "print(\"\\nMissing values in key fields:\")\n",
    "print(df_all[['title', 'publish_time', 'trending_date', 'category_name']].isnull().sum())\n",
    "\n",
    "# Quick look at derived features\n",
    "print(\"\\nHas emoji breakdown:\")\n",
    "print(df_all['has_emoji'].value_counts())\n",
    "\n",
    "print(\"\\nAverage title length per country:\")\n",
    "print(df_all.groupby('country')['title_length'].mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff2ad92",
   "metadata": {},
   "source": [
    "### Step 8: Final Remarks and Next Steps\n",
    "\n",
    "The data cleaning and enrichment process has been completed successfully across all ten countries. Each dataset has been:\n",
    "\n",
    "- Parsed for accurate datetime values\n",
    "- Enriched with time-based and text-based features\n",
    "- Mapped to human-readable category names\n",
    "- Consolidated into a single master dataset with a `country` label\n",
    "\n",
    "#### Key Takeaways from the Validation:\n",
    "- No missing values in critical fields\n",
    "- Emoji usage is rare overall but present (~3%)\n",
    "- Title lengths vary meaningfully by country â€” suggesting cultural or platform strategy differences\n",
    "\n",
    "The final cleaned dataset (`clean_data/all_countries_clean.csv`) is now ready for visualization in Tableau. The next phase will focus on exploring how audience preferences have evolved over time â€” across content types, formats, and countries â€” to better understand the cultural pulse of YouTube.\n",
    "\n",
    "ðŸ‘‰ Proceed to Tableau for interactive analysis and storytelling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
